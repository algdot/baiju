<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>白驹</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="keywords" content="java nlp">
<meta property="og:type" content="website">
<meta property="og:title" content="白驹">
<meta property="og:url" content="https://day6.github.io/index.html">
<meta property="og:site_name" content="白驹">
<meta property="og:locale" content="zh">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="白驹">
  
    <link rel="alternate" href="/baiju/atom.xml" title="白驹" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/baiju/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/baiju/" id="logo">白驹</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/baiju/">Home</a>
        
          <a class="main-nav-link" href="/baiju/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/baiju/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://day6.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-task4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/baiju/2019/05/16/task4/" class="article-date">
  <time datetime="2019-05-16T13:56:31.000Z" itemprop="datePublished">2019-05-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/baiju/2019/05/16/task4/">task4</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Task4"><a href="#Task4" class="headerlink" title="Task4"></a>Task4</h1><h2 id="词袋模型（BOW，bag-of-words）"><a href="#词袋模型（BOW，bag-of-words）" class="headerlink" title="词袋模型（BOW，bag of words）"></a>词袋模型（BOW，bag of words）</h2><p>Jane wants to go to Shenzhen.</p>
<p>Bob  wants to go to Shanghai.</p>
<pre><code>将所有词语装进一个袋子里，不考虑其词法和语序的问题，即每个词语都是独立的。例如上面2个例句，就可以构成一个词袋，袋子里包括Jane、wants、to、go、Shenzhen、Bob、Shanghai。假设建立一个数组（或词典）用于映射匹配
</code></pre><p>[Jane, wants, to, go, Shenzhen, Bob, Shanghai]</p>
<pre><code>那么上面两个例句就可以用以下两个向量表示，对应的下标与映射数组的下标相匹配，其值为该词语出现的次数
</code></pre><p>[1,1,2,1,1,0,0]<br>[0,1,2,1,0,1,1]</p>
<pre><code>这两个词频向量就是词袋模型，可以很明显的看到语序关系已经完全丢失。
</code></pre><h2 id="词向量模型"><a href="#词向量模型" class="headerlink" title="词向量模型"></a>词向量模型</h2><pre><code>词向量模型是考虑词语位置关系的一种模型。通过大量语料的训练，将每一个词语映射到高维度（几千、几万维以上）的向量当中，通过求余弦的方式，可以判断两个词语之间的关系，例如例句中的Jane和Bob在词向量模型中，他们的余弦值可能就接近1，因为这两个都是人名，Shenzhen和Bob的余弦值可能就接近0，因为一个是人名一个是地名。
现在常用word2vec构成词向量模型，它的底层采用基于CBOW和Skip-Gram算法的神经网络模型。
</code></pre><h2 id="Skip-gram-和-CBOW-模型"><a href="#Skip-gram-和-CBOW-模型" class="headerlink" title="Skip-gram 和 CBOW 模型"></a>Skip-gram 和 CBOW 模型</h2><ul>
<li>如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』</li>
<li>而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』</li>
</ul>
<h2 id="word2vec1"><a href="#word2vec1" class="headerlink" title="word2vec1"></a>word2vec1</h2><h3 id="gensim"><a href="#gensim" class="headerlink" title="gensim"></a>gensim</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from gensim.models import word2vec</span><br><span class="line">sentences = word2vec.LineSentence(&apos;./in_the_name_of_people_segment.txt&apos;)</span><br><span class="line">model = word2vec.Word2Vec(sentences, hs=1,min_count=1,window=3,size=100)</span><br><span class="line">model.wv.similar_by_word(&apos;沙瑞金&apos;.decode(&apos;utf-8&apos;), topn =100)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://blog.csdn.net/sinat_36521655/article/details/79993369" target="_blank" rel="noopener">词袋模型（BOW，bag of words）和词向量模型（Word Embedding）概念介绍</a></li>
<li><a href="https://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">word2vec原理(一) CBOW与Skip-Gram模型基础</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/26306795" target="_blank" rel="noopener">秒懂词向量Word2vec的本质</a></li>
<li><a href="https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/examples/tutorials/word2vec/word2vec_basic.py" target="_blank" rel="noopener">tensorflow word2vec example</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://day6.github.io/2019/05/16/task4/" data-id="cjvtkaqx70004s7fy3didjmh1" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-task3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/baiju/2019/05/16/task3/" class="article-date">
  <time datetime="2019-05-16T12:32:44.395Z" itemprop="datePublished">2019-05-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="task3"><a href="#task3" class="headerlink" title="task3"></a>task3</h1><h2 id="TF-IDF概述"><a href="#TF-IDF概述" class="headerlink" title="TF-IDF概述"></a>TF-IDF概述</h2><p>TF-IDF是Term Frequency -  Inverse Document Frequency的缩写，即“词频-逆文本频率”。它由两部分组成，TF和IDF。</p>
<h2 id="tf-idf-方法1"><a href="#tf-idf-方法1" class="headerlink" title="tf-idf 方法1"></a>tf-idf 方法1</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfTransformer  </span><br><span class="line">from sklearn.feature_extraction.text import CountVectorizer  </span><br><span class="line"></span><br><span class="line">corpus=[&quot;I come to China to travel&quot;, </span><br><span class="line">    &quot;This is a car polupar in China&quot;,          </span><br><span class="line">    &quot;I love tea and Apple &quot;,   </span><br><span class="line">    &quot;The work is to write some papers in science&quot;] </span><br><span class="line"></span><br><span class="line">vectorizer=CountVectorizer()</span><br><span class="line"></span><br><span class="line">transformer = TfidfTransformer()</span><br><span class="line">tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))  </span><br><span class="line">print(tfidf)</span><br></pre></td></tr></table></figure>
<h2 id="tf-idf-方法2"><a href="#tf-idf-方法2" class="headerlink" title="tf-idf 方法2"></a>tf-idf 方法2</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">tfidf2 = TfidfVectorizer()</span><br><span class="line">re = tfidf2.fit_transform(corpus)</span><br><span class="line">print(re)</span><br></pre></td></tr></table></figure>
<h2 id="tf-idf-方法3"><a href="#tf-idf-方法3" class="headerlink" title="tf-idf 方法3"></a>tf-idf 方法3</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">def tf(word, count):</span><br><span class="line">    return count[word] / sum(count.values())</span><br><span class="line">def n_containing(word, count_list):</span><br><span class="line">    return sum(1 for count in count_list if word in count)</span><br><span class="line">def idf(word, count_list):</span><br><span class="line">    return math.log(len(count_list)) / (1 + n_containing(word, count_list))</span><br><span class="line">def tfidf(word, count, count_list):</span><br><span class="line">    return tf(word, count) * idf(word, count_list)</span><br><span class="line"></span><br><span class="line">def get_tokens(text):</span><br><span class="line">    lower = text.lower()</span><br><span class="line">    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)</span><br><span class="line">    no_punctuation = lower.translate(remove_punctuation_map)</span><br><span class="line">    tokens = nltk.word_tokenize(no_punctuation)</span><br><span class="line"></span><br><span class="line">    return tokens</span><br><span class="line"></span><br><span class="line">def count_term(text):</span><br><span class="line">    tokens = get_tokens(text)</span><br><span class="line">    filtered = [w for w in tokens if not w in stopwords.words(&apos;english&apos;)]</span><br><span class="line">    stemmer = PorterStemmer()</span><br><span class="line">    stemmed = stem_tokens(filtered, stemmer)</span><br><span class="line">    count = Counter(stemmed)</span><br><span class="line">    return count</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    text1 = &quot;Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora. Challenges in natural language processing frequently involve natural language understanding, natural language generation (frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.&quot;</span><br><span class="line"></span><br><span class="line">    text2 = &quot;The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.&quot;</span><br><span class="line"></span><br><span class="line">    text3 = &quot;During the 1970s, many programmers began to write conceptual ontologies, which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky。&quot;</span><br><span class="line"></span><br><span class="line">    texts = [text1, text2, text3]</span><br><span class="line">    countlist = []</span><br><span class="line">    for text in texts:</span><br><span class="line">        countlist.append(count_term(text))</span><br><span class="line">    for i, count in enumerate(countlist):</span><br><span class="line">        print(&quot;Top words in document &#123;&#125;&quot;.format(i + 1))</span><br><span class="line">        scores = &#123;word: tfidf(word, count, countlist) for word in count&#125;</span><br><span class="line">        sorted_words = sorted(scores.items(), key = lambda x: x[1], reverse=True)</span><br><span class="line">        for word, score in sorted_words[:5]:</span><br><span class="line">            print(&quot;\tWord: &#123;&#125;, TF-IDF: &#123;&#125;&quot;.format(word, round(score, 5)))</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h2 id="点互信息和互信息"><a href="#点互信息和互信息" class="headerlink" title="点互信息和互信息"></a>点互信息和互信息</h2><p>  机器学习相关文献里面，经常会用到点互信息PMI(Pointwise Mutual Information)这个指标来衡量两个事物之间的相关性（比如两个词）。<br>  <img src="/2019/05/16/task3/pmi.png" alt="pmi"></p>
<p>  在概率论中，我们知道，如果x跟y不相关，则p(x,y)=p(x)p(y)。二者相关性越大，则p(x, y)就相比于p(x)p(y)越大。用后面的式子可能更好理解，在y出现的情况下x出现的条件概率p(x|y)除以x本身出现的概率p(x)，自然就表示x跟y的相关程度。</p>
<p>  举个自然语言处理中的例子来说，我们想衡量like这个词的极性（正向情感还是负向情感）。我们可以预先挑选一些正向情感的词，比如good。然后我们算like跟good的PMI。</p>
<p>  点互信息PMI其实就是从信息论里面的互信息这个概念里面衍生出来的。<br>  <img src="/2019/05/16/task3/mi.png" alt="mi"></p>
<p>  其衡量的是两个随机变量之间的相关性，即一个随机变量中包含的关于另一个随机变量的信息量。所谓的随机变量，即随机试验结果的量的表示，可以简单理解为按照一个概率分布进行取值的变量，比如随机抽查的一个人的身高就是一个随机变量。可以看出，互信息其实就是对X和Y的所有可能的取值情况的点互信息PMI的加权和。因此，点互信息这个名字还是很形象的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import metrics as mr</span><br><span class="line">mr.mutual_info_score(label,x)</span><br></pre></td></tr></table></figure>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/26766008" target="_blank" rel="noopener">TF-IDF的算法Python实现和简单示例</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://day6.github.io/2019/05/16/task3/" data-id="cjvtkaqx40002s7fycq875h4o" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-task2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/baiju/2019/05/16/task2/" class="article-date">
  <time datetime="2019-05-16T12:32:30.328Z" itemprop="datePublished">2019-05-16</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Task2"><a href="#Task2" class="headerlink" title="Task2"></a>Task2</h1><h2 id="jieba分词"><a href="#jieba分词" class="headerlink" title="jieba分词"></a>jieba分词</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line"># -*- coding: cp936 -*-</span><br><span class="line">import jieba</span><br><span class="line">import jieba.analyse</span><br><span class="line"></span><br><span class="line">f = open(&apos;it168.txt&apos;,&apos;r&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line">text = f.read()</span><br><span class="line">f.close()</span><br><span class="line"></span><br><span class="line"># 分词</span><br><span class="line">seg_list = jieba.cut(text, cut_all=True)</span><br><span class="line">print(&quot;Full Mode: &quot; + &quot;/ &quot;.join(seg_list))  # 全模式</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut(text, cut_all=False)</span><br><span class="line">print(&quot;Default Mode: &quot; + &quot;/ &quot;.join(seg_list))  # 精确模式</span><br><span class="line"></span><br><span class="line">seg_list = jieba.cut_for_search(text)  # 搜索引擎模式</span><br><span class="line">print(&quot;, &quot;.join(seg_list))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 关键字提取</span><br><span class="line"># 基于TF-IDF算法的关键词抽取</span><br><span class="line"># sentence 为待提取的文本</span><br><span class="line"># topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20</span><br><span class="line"># withWeight 为是否一并返回关键词权重值，默认值为 False</span><br><span class="line"># allowPOS 仅包括指定词性的词，默认值为空，即不筛选</span><br><span class="line">keywords = jieba.analyse.extract_tags(sentence=text, topK=20, withWeight=True, allowPOS=(&apos;n&apos;,&apos;nr&apos;,&apos;ns&apos;))</span><br><span class="line"># 基于TextRank算法的关键词抽取</span><br><span class="line"># keywords = jieba.analyse.textrank(text, topK=20, withWeight=True, allowPOS=(&apos;n&apos;,&apos;nr&apos;,&apos;ns&apos;))</span><br><span class="line">for item in keywords:</span><br><span class="line">    print(item[0],item[1])</span><br><span class="line"># 词语标注</span><br><span class="line">import jieba.posseg</span><br><span class="line"># 新建自定义分词器，tokenizer 参数可指定内部使用的 jieba.Tokenizer 分词器。jieba.posseg.dt 为默认词性标注分词器。</span><br><span class="line">posseg = jieba.posseg.POSTokenizer(tokenizer=None)</span><br><span class="line">words = posseg.cut(text)</span><br><span class="line">for word, flag in words:</span><br><span class="line">    print(&apos;%s %s&apos; % (word, flag))</span><br><span class="line"></span><br><span class="line"># 对原有的语料库添加词语</span><br><span class="line">jieba.add_word(word, freq=None, tag=None)</span><br><span class="line"># 导入语料文件</span><br><span class="line">jieba.load_userdict(&apos;disney.txt&apos;)</span><br><span class="line">(END)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://day6.github.io/2019/05/16/task2/" data-id="cjvtkaqx60003s7fyue68hpjy" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-task1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/baiju/2019/05/12/task1/" class="article-date">
  <time datetime="2019-05-12T06:23:44.167Z" itemprop="datePublished">2019-05-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Task1"><a href="#Task1" class="headerlink" title="Task1"></a>Task1</h1><h2 id="IMDB"><a href="#IMDB" class="headerlink" title="IMDB"></a>IMDB</h2><p>  使用神经网络，对文本形式的影评分为“正面”或“负面”影评</p>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><ul>
<li>下载数据<ul>
<li>保留出现频次在前 10000 位的字词</li>
</ul>
</li>
<li>探索数据<ul>
<li>把字词转为整数数组</li>
</ul>
</li>
<li>准备数据<ul>
<li>文字转数字</li>
<li>填充(keras.preprocessing.sequence.pad_sequences)</li>
</ul>
</li>
<li>构建模型</li>
<li>创建验证集</li>
<li>评估模型</li>
<li>观察训练过程指标</li>
</ul>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><pre><code>* 周期:20 [0.30585839036941526, 0.87532]
* 周期:40 [0.32981066118240354, 0.87176]
* 周期:60 [0.4299388340139389, 0.86172]
结论：训练周期越多损失越大，准确度不一定增加。
问题：那损失和准确度什么时候最好？
</code></pre><h3 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h3><p>  参数异常: 向量不在[0, 10000)范围内<br>  错误信息:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensorflow.python.framework.errors_impl.InvalidArgumentError: indices[128,27] = 11964 is not in [0, 10000)</span><br><span class="line">vocab_size = 15000</span><br></pre></td></tr></table></figure></p>
<p>  错误原因:<br>    选取的字词为15000, 训练时却只设置了10000</p>
<h2 id="THUCNews"><a href="#THUCNews" class="headerlink" title="THUCNews"></a>THUCNews</h2><p>  使用CNN与RNN中文文本分类</p>
<ul>
<li>缺少中文分词</li>
</ul>
<h2 id="其他知识"><a href="#其他知识" class="headerlink" title="其他知识"></a>其他知识</h2><h3 id="sklearn-metrics使用"><a href="#sklearn-metrics使用" class="headerlink" title="sklearn.metrics使用"></a>sklearn.metrics使用</h3><pre><code>TP（True Positive）：在判定为positive的样本中，判断正确的数目。
FP（False Positive）：在判定为positive的样本中，判断错误的数目。
TN（True Negative）：在判定为negative的样本中，判断正确的数目。
FN（False Negative）：在判定为negative的样本中，判断错误的数目。
</code></pre><ul>
<li><p>precision_score: 精确率 P = TP/(TP+FP), 可理解为”真正属于类别P的/找到属于类别P的”</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.precision_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sample_weight=None)</span><br></pre></td></tr></table></figure>
</li>
<li><p>recall_score: 召回率 R = TP/(TP+FN), 可理解为”真正属于类别P的/所有属于类别P的”，召回率1最好，0最差</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.recall_score(y_true, y_pred, labels=None, pos_label=1, average=’binary’, sample_weight=None)</span><br></pre></td></tr></table></figure>
</li>
<li><p>accuracy_score: 准确率 A = (TP+TN)/(TP+TF+TN+TF)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.accuracy_score(y_true, y_pred, normalize=True, sample_weight=None)</span><br></pre></td></tr></table></figure>
</li>
<li><p>f1_score</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F1 = 2 * (precision * recall) / (precision + recall)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="ROC曲线和AUC"><a href="#ROC曲线和AUC" class="headerlink" title="ROC曲线和AUC"></a>ROC曲线和AUC</h3><p>  ROC（Receiver Operating Characteristic）和AUC（Area UnderCharacteristic）常被用来评价一个二分类器的优劣。</p>
<p>  ROC曲线一般横轴是FPR，纵轴是TPR。AUC为曲线下面的面积，一般AUC值越大，说明模型越好。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul>
<li><a href="https://tensorflow.google.cn/tutorials/keras/basic_text_classification#create_a_validation_set" target="_blank" rel="noopener">影评文本分类</a></li>
<li><a href="https://gaussic.github.io/2017/08/30/text-classification-tensorflow/" target="_blank" rel="noopener">CNN与RNN中文文本分类-基于TENSORFLOW实现</a></li>
<li><a href="https://blog.csdn.net/index20001/article/details/77651028" target="_blank" rel="noopener">机器学习中的precision, recall, accuracy, F值</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://day6.github.io/2019/05/12/task1/" data-id="cjvtkaqx20001s7fyfef6n141" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="i-u" class="article article-type-i" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/baiju/2018/04/27/u/" class="article-date">
  <time datetime="2018-04-26T16:31:38.000Z" itemprop="datePublished">2018-04-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/baiju/2018/04/27/u/">消息队列</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h1><p>  Producer把消息按照顺序放入队列中，由Consumer异步消费, 从而达到事务的一致性</p>
<ul>
<li>组成部分<ul>
<li>Producer</li>
<li>Consumer</li>
<li>Queue</li>
</ul>
</li>
<li>特性<ul>
<li>路由</li>
<li>订阅模式</li>
<li>消息轨迹</li>
<li>消费回放</li>
</ul>
</li>
</ul>
<h2 id="RocketMq"><a href="#RocketMq" class="headerlink" title="RocketMq"></a>RocketMq</h2><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>  组成部分</p>
<ul>
<li>NameService</li>
<li>Broker</li>
<li>Producer</li>
<li>Consumer<br>架构图</li>
</ul>
<h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul>
<li>定时消息</li>
</ul>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><h2 id="RabbitMq"><a href="#RabbitMq" class="headerlink" title="RabbitMq"></a>RabbitMq</h2><h3 id="架构-1"><a href="#架构-1" class="headerlink" title="架构"></a>架构</h3><ul>
<li></li>
</ul>
<h2 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h2><ul>
<li>优先队列（priority)</li>
</ul>
<h2 id="nginx"><a href="#nginx" class="headerlink" title="nginx"></a>nginx</h2><p>auto/configure –sbin-path=/export/nginx-dev/nginx –conf-path=/export/nginx-dev/nginx.conf –pid-path=/export/nginx-dev/nginx.pid –with-http_ssl_module –with-debug –with-openssl=/usr/local/Cellar/openssl/1.0.2o_1</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://day6.github.io/2018/04/27/u/" data-id="cjvtkaqx90005s7fyye0icgi6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/baiju/2018/04/27/hello-world/" class="article-date">
  <time datetime="2018-04-26T16:14:21.000Z" itemprop="datePublished">2018-04-27</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/baiju/2018/04/27/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://day6.github.io/2018/04/27/hello-world/" data-id="cjvtkaqwu0000s7fy5ihs482t" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
</article>


  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/baiju/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/baiju/archives/2018/04/">April 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/baiju/2019/05/16/task4/">task4</a>
          </li>
        
          <li>
            <a href="/baiju/2019/05/16/task3/">(no title)</a>
          </li>
        
          <li>
            <a href="/baiju/2019/05/16/task2/">(no title)</a>
          </li>
        
          <li>
            <a href="/baiju/2019/05/12/task1/">(no title)</a>
          </li>
        
          <li>
            <a href="/baiju/2018/04/27/u/">消息队列</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 baiju<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/baiju/" class="mobile-nav-link">Home</a>
  
    <a href="/baiju/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/baiju/fancybox/jquery.fancybox.css">
  <script src="/baiju/fancybox/jquery.fancybox.pack.js"></script>


<script src="/baiju/js/script.js"></script>



  </div>
</body>
</html>